#!/bin/sh
# set -e

# Triton specific model_repository
# model_repository
# └── vllm_model
#    ├── 1
#    │   └── model.json
#    └── config.pbtxt

MODEL_REPOSITORY="${MODEL_REPOSITORY:-/opt/app-root/model_repository}"

# models
# └── meta-llama
#     ├── Llama-2-7b-hf
#     │   └── model files

MODEL_CACHE="${MODEL_CACHE:-/opt/app-root/models}"


usage(){
  "${STI_SCRIPTS_PATH}/usage"
}

# fallback with sleep to debug your code
# oc rsh / podman exec -it .. /bin/sh
run_sleep(){
  echo "Ponder the universe with infinate sleep..."
  sleep infinity
}

# run normal s2i scripts
run_default(){
  NVIDIA_ENTRYPOINT=/opt/nvidia/nvidia_entrypoint.sh

  [ -e /NGC-DL-CONTAINER-LICENSE ] && cat /NGC-DL-CONTAINER-LICENSE

  if [ -e "${NVIDIA_ENTRYPOINT}" ]; then
    "${NVIDIA_ENTRYPOINT}" "$@"
  else
    exec "$@"
  fi
}

run_init(){
  echo "MODEL_REPOSITORY=${MODEL_REPOSITORY}
  "

  if [ -d "${MODEL_REPOSITORY}" ]; then
    echo "path exists: ${MODEL_REPOSITORY}"
  else
    echo "creating model repository at: ${MODEL_REPOSITORY}"
    mkdir -p "${MODEL_REPOSITORY}"
  fi
}

run_scripts(){
  [ -d /scripts ] || return
  echo "found: /scripts"

  for script in /scripts/*.sh
  do
    echo "running: $script"
    "$script"
  done
}

run_triton(){
  # export TRANSFORMERS_CACHE=/models/
  # export TRITON_CLOUD_CREDENTIAL_PATH="/opt/cloud_credential.json"

  # https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_repository.html?highlight=aws_access_key_id#s3
  # export TRITON_AWS_MOUNT_DIRECTORY=...

  which tritonserver || return

  DEFAULT_TRITON_ARGS='--model-control-mode=poll --repository-poll-secs=60'
  TRITON_ARGS=${TRITON_ARGS:-${DEFAULT_TRITON_ARGS}}

  # ENTRYPOINT ["/opt/nvidia/nvidia_entrypoint.sh", "tritonserver", "--model-repository", "/triton-inference-server/docs/examples/model_repository"]
  tritonserver \
    --model-repository="${MODEL_REPOSITORY}" \
    ${TRITON_ARGS}
}


usage

run_init
run_scripts
run_triton
# run_default "$@"
run_sleep
