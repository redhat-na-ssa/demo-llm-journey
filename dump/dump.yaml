apiVersion: v1
items:
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      alpha.image.policy.openshift.io/resolve-names: '*'
      app.openshift.io/route-disabled: "false"
      deployment.kubernetes.io/revision: "3"
      image.openshift.io/triggers: '[{"from":{"kind":"ImageStreamTag","name":"triton-vllm-inference-server:latest","namespace":"triton-vllm-inference-server"},"fieldPath":"spec.template.spec.containers[?(@.name==\"triton-vllm-inference-server\")].image","pause":"false"}]'
      openshift.io/generated-by: OpenShiftWebConsole
    creationTimestamp: "2024-06-24T22:49:54Z"
    generation: 3
    labels:
      app: triton-vllm-inference-server
      app.kubernetes.io/component: triton-vllm-inference-server
      app.kubernetes.io/instance: triton-vllm-inference-server
      app.kubernetes.io/name: triton-vllm-inference-server
      app.kubernetes.io/part-of: triton-vllm-inference-server-app
      app.openshift.io/runtime-namespace: triton-vllm-inference-server
    name: triton-vllm-inference-server
    namespace: triton-vllm-inference-server
    resourceVersion: "521960"
    uid: 831074c4-daad-456c-a1f5-371a05cd9b72
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: triton-vllm-inference-server
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          openshift.io/generated-by: OpenShiftWebConsole
        creationTimestamp: null
        labels:
          app: triton-vllm-inference-server
          deployment: triton-vllm-inference-server
      spec:
        containers:
        - image: image-registry.openshift-image-registry.svc:5000/triton-vllm-inference-server/triton-vllm-inference-server@sha256:a7a82f451ee052f8e1e9fc0b770c54c9efa12b5ae723447922ef8e358e54fd22
          imagePullPolicy: Always
          name: triton-vllm-inference-server
          ports:
          - containerPort: 8000
            protocol: TCP
          - containerPort: 8001
            protocol: TCP
          - containerPort: 8002
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/app-root/models
            name: model-cache
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: model-cache
          persistentVolumeClaim:
            claimName: model-cache
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2024-06-25T00:46:40Z"
      lastUpdateTime: "2024-06-25T00:46:40Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-06-24T22:49:54Z"
      lastUpdateTime: "2024-06-25T16:37:53Z"
      message: ReplicaSet "triton-vllm-inference-server-c9c7d6c4c" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 3
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      openshift.io/generated-by: OpenShiftWebConsole
    creationTimestamp: "2024-06-24T22:49:54Z"
    labels:
      app: triton-vllm-inference-server
      app.kubernetes.io/component: triton-vllm-inference-server
      app.kubernetes.io/instance: triton-vllm-inference-server
      app.kubernetes.io/name: triton-vllm-inference-server
      app.kubernetes.io/part-of: triton-vllm-inference-server-app
      app.openshift.io/runtime-version: latest
    name: triton-vllm-inference-server
    namespace: triton-vllm-inference-server
    resourceVersion: "91826"
    uid: e5371709-911e-4f15-969b-40183147e6d1
  spec:
    clusterIP: 172.30.220.155
    clusterIPs:
    - 172.30.220.155
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: 8000-tcp
      port: 8000
      protocol: TCP
      targetPort: 8000
    - name: 8001-tcp
      port: 8001
      protocol: TCP
      targetPort: 8001
    - name: 8002-tcp
      port: 8002
      protocol: TCP
      targetPort: 8002
    selector:
      app: triton-vllm-inference-server
      deployment: triton-vllm-inference-server
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: image.openshift.io/v1
  kind: ImageStream
  metadata:
    annotations:
      openshift.io/image.dockerRepositoryCheck: "2024-06-24T22:49:54Z"
    creationTimestamp: "2024-06-24T22:49:54Z"
    generation: 2
    labels:
      app: triton-vllm-inference-server
      app.kubernetes.io/component: triton-vllm-inference-server
      app.kubernetes.io/instance: triton-vllm-inference-server
      app.kubernetes.io/name: triton-vllm-inference-server
      app.kubernetes.io/part-of: triton-vllm-inference-server-app
    name: triton-vllm-inference-server
    namespace: triton-vllm-inference-server
    resourceVersion: "91824"
    uid: 70147e72-9f21-43ce-ae87-14aa1912402f
  spec:
    lookupPolicy:
      local: false
    tags:
    - annotations:
        openshift.io/generated-by: OpenShiftWebConsole
        openshift.io/imported-from: quay.io/carl_mes/triton-vllm-inference-server:latest
      from:
        kind: DockerImage
        name: quay.io/carl_mes/triton-vllm-inference-server:latest
      generation: 2
      importPolicy:
        importMode: Legacy
      name: latest
      referencePolicy:
        type: Local
  status:
    dockerImageRepository: image-registry.openshift-image-registry.svc:5000/triton-vllm-inference-server/triton-vllm-inference-server
    tags:
    - items:
      - created: "2024-06-24T22:49:54Z"
        dockerImageReference: quay.io/carl_mes/triton-vllm-inference-server@sha256:a7a82f451ee052f8e1e9fc0b770c54c9efa12b5ae723447922ef8e358e54fd22
        generation: 2
        image: sha256:a7a82f451ee052f8e1e9fc0b770c54c9efa12b5ae723447922ef8e358e54fd22
      tag: latest
- apiVersion: v1
  kind: PersistentVolumeClaim
  metadata:
    annotations:
      pv.kubernetes.io/bind-completed: "yes"
      pv.kubernetes.io/bound-by-controller: "yes"
      volume.beta.kubernetes.io/storage-provisioner: openshift-storage.cephfs.csi.ceph.com
      volume.kubernetes.io/storage-provisioner: openshift-storage.cephfs.csi.ceph.com
    creationTimestamp: "2024-06-25T16:35:02Z"
    finalizers:
    - kubernetes.io/pvc-protection
    name: model-cache
    namespace: triton-vllm-inference-server
    resourceVersion: "520473"
    uid: 2d0cd0fa-6ff3-4b28-b165-1e3562dd05a6
  spec:
    accessModes:
    - ReadWriteMany
    resources:
      requests:
        storage: 200Gi
    storageClassName: ocs-storagecluster-cephfs
    volumeMode: Filesystem
    volumeName: pvc-2d0cd0fa-6ff3-4b28-b165-1e3562dd05a6
  status:
    accessModes:
    - ReadWriteMany
    capacity:
      storage: 200Gi
    phase: Bound
- apiVersion: route.openshift.io/v1
  kind: Route
  metadata:
    creationTimestamp: "2024-06-24T22:49:54Z"
    labels:
      app: triton-vllm-inference-server
      app.kubernetes.io/component: triton-vllm-inference-server
      app.kubernetes.io/instance: triton-vllm-inference-server
      app.kubernetes.io/name: triton-vllm-inference-server
      app.kubernetes.io/part-of: triton-vllm-inference-server-app
      app.openshift.io/runtime-version: latest
    name: triton-vllm-inference-server
    namespace: triton-vllm-inference-server
    resourceVersion: "91844"
    uid: 1e1e372f-3af4-41c6-add3-1dff26ae835f
  spec:
    host: triton-rest-api
    port:
      targetPort: 8000-tcp
    tls:
      insecureEdgeTerminationPolicy: Redirect
      termination: edge
    to:
      kind: Service
      name: triton-vllm-inference-server
      weight: 100
    wildcardPolicy: None
  status:
    ingress:
    - conditions:
      - lastTransitionTime: "2024-06-24T22:49:54Z"
        status: "True"
        type: Admitted
      host: triton-rest-api
      routerCanonicalHostname: router-default.apps.cluster-72j78.72j78.sandbox61.opentlc.com
      routerName: default
      wildcardPolicy: None
kind: List
metadata:
  resourceVersion: ""
