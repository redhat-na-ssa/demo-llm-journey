kind: ConfigMap
apiVersion: v1
metadata:
  labels:
    app: triton-vllm-inference-server
    app.kubernetes.io/component: triton-vllm-inference-server
    app.kubernetes.io/instance: triton-vllm-inference-server
    app.kubernetes.io/name: triton-vllm-inference-server
    app.kubernetes.io/part-of: triton-vllm-inference-server-app
    app.openshift.io/runtime-namespace: triton-vllm-inference-server
  name: triton-vllm-inference-server
immutable: false
data:
  config.pbtxt: |
    backend: "vllm"

    instance_group [
      {
        count: 1
        kind: KIND_MODEL
      }
    ]

    model_transaction_policy {
      decoupled: True
    }

    max_batch_size: 0

    # https://github.com/triton-inference-server/server/issues/6578#issuecomment-1813112797
    # Note: The vLLM backend uses the following input and output names.
    # Any change here needs to also be made in model.py

    input [
      {
        name: "text_input"
        data_type: TYPE_STRING
        dims: [ 1 ]
      },
      {
        name: "stream"
        data_type: TYPE_BOOL
        dims: [ 1 ]
      },
      {
        name: "sampling_parameters"
        data_type: TYPE_STRING
        dims: [ 1 ]
        optional: true
      }
    ]

    output [
      {
        name: "text_output"
        data_type: TYPE_STRING
        dims: [ -1 ]
      }
    ]

  model.json: |

    {
        "model": "/opt/app-root/mnt/model_repository/vllm_model/Llama-2-7b-hf",
        "disable_log_requests": "true",
        "gpu_memory_utilization": 1
    }
